{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href=\"https://fonts.googleapis.com/css?family=Cabin|Quicksand\" rel=\"stylesheet\"><style>.container{width:90% !important; font-family: \"Cabin\", sans-serif;}em{color: red !important;}</style><style>.output_png {display: table-cell;text-align: center;vertical-align: middle;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sympy import init_printing \n",
    "from sympy import Matrix\n",
    "init_printing(use_latex=True)\n",
    "def out(mat, n=2): return Matrix(np.round(mat, decimals=n))\n",
    "from IPython.core.display import HTML\n",
    "HTML('<link href=\"https://fonts.googleapis.com/css?family=Cabin|Quicksand\" rel=\"stylesheet\"><style>.container{width:90% !important; font-family: \"Cabin\", sans-serif;}em{color: red !important;}</style><style>.output_png {display: table-cell;text-align: center;vertical-align: middle;}</style>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Covariance\n",
    "\n",
    "- the covariance is a measure of the joint variability of two random variables\n",
    "- if the variables tend to show similar behavior the covariance is positive\n",
    "  - if the greater values of one variable mainly correspond with the greater values of the other variable\n",
    "  - and the same holds for the lesser values\n",
    "- the sign of the covariance therefore shows the tendency in the *linear* relationship between the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Covariance\n",
    "\n",
    "- the covariance between two jointly distributed real-valued random variables X and Y is defined as the expected product of their deviations from their individual expected values\n",
    "\n",
    "$$\\operatorname {cov} (X,Y)=\\operatorname {E} {{\\big [}(X-\\operatorname {E} [X])(Y-\\operatorname {E} [Y]){\\big ]}}$$\n",
    "\n",
    "- it is also denoted $σ_{XY}$ or $σ(X,Y)$ in analogy to variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Linearity of expectation\n",
    "\n",
    "- The expected value operator (or expectation operator) $\\operatorname {E}[\\cdot ]$ is linear \n",
    "    - for any random variables $X$ and $Y$ and constant $a$\n",
    "\n",
    "$${\\displaystyle {\\begin{aligned}\\operatorname {E} [X+Y]&=\\operatorname {E} [X]+\\operatorname {E} [Y]\n",
    "\\\\\\operatorname {E} [aX]&=a\\operatorname {E} [X]\n",
    "\\end{aligned}}}$$\n",
    "\n",
    "\n",
    "So:\n",
    "- the expected value of the sum of random variables is the sum of the expected values\n",
    "- and the expected value scales linearly with a multiplicative scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Covariance formulation\n",
    "\n",
    "$${\\displaystyle {\\begin{aligned}\\operatorname {cov} (X,Y)&=\\operatorname {E} \\left[\\left(X-\\operatorname {E} \\left[X\\right]\\right)\\left(Y-\\operatorname {E} \\left[Y\\right]\\right)\\right]\\\\&=\\operatorname {E} \\left[XY-X\\operatorname {E} \\left[Y\\right]-\\operatorname {E} \\left[X\\right]Y+\\operatorname {E} \\left[X\\right]\\operatorname {E} \\left[Y\\right]\\right]\\\\&=\\operatorname {E} \\left[XY\\right]-\\operatorname {E} \\left[X\\right]\\operatorname {E} \\left[Y\\right]-\\operatorname {E} \\left[X\\right]\\operatorname {E} \\left[Y\\right]+\\operatorname {E} \\left[X\\right]\\operatorname {E} \\left[Y\\right]\\\\&=\\operatorname {E} \\left[XY\\right]-\\operatorname {E} \\left[X\\right]\\operatorname {E} \\left[Y\\right]\\end{aligned}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Properties of covariance\n",
    "\n",
    "- ${\\displaystyle \\operatorname {cov} (X,X)=\\operatorname {var} (X)\\equiv \\sigma ^{2}(X)\\equiv \\sigma _{X}^{2}}$\n",
    "- when $X, Y, W$ and $V$ are real-valued random variables and $a, b, c, d$ are constants\n",
    "\n",
    "$${\\displaystyle {\\begin{aligned}\\operatorname {cov} (X,a)&=0\\\\\\operatorname {cov} (X,X)&=\\operatorname {var} (X)\\\\\\operatorname {cov} (X,Y)&=\\operatorname {cov} (Y,X)\\\\\\operatorname {cov} (aX,bY)&=ab\\,\\operatorname {cov} (X,Y)\\\\\\operatorname {cov} (X+a,Y+b)&=\\operatorname {cov} (X,Y)\\\\\\operatorname {cov} (aX+bY,cW+dV)&=ac\\,\\operatorname {cov} (X,W)+ad\\,\\operatorname {cov} (X,V)+bc\\,\\operatorname {cov} (Y,W)+bd\\,\\operatorname {cov} (Y,V)\\end{aligned}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Cross covariance matrix\n",
    "\n",
    "- for random vectors $\\mathbf {X} \\in \\mathbb {R} ^{m}$ and $\\mathbf {Y} \\in \\mathbb {R} ^{n}$\n",
    "- the $m × n$ cross covariance matrix is equal to\n",
    "\n",
    "$${\\begin{aligned}\\operatorname {cov} (\\mathbf {X} ,\\mathbf {Y} )&=\\operatorname {E} \\left[(\\mathbf {X} -\\operatorname {E} [\\mathbf {X} ])(\\mathbf {Y} -\\operatorname {E} [\\mathbf {Y} ])^{\\mathrm {T} }\\right]\\\\&=\\operatorname {E} \\left[\\mathbf {X} \\mathbf {Y} ^{\\mathrm {T} }\\right]-\\operatorname {E} [\\mathbf {X} ]\\operatorname {E} [\\mathbf {Y} ]^{\\mathrm {T} }\\end{aligned}}$$\n",
    "\n",
    "- the (i, j)-th element of this matrix is equal to the covariance $\\operatorname {cov}(X_i, Y_j)$ between the i-th scalar component of $X$ and the j-th scalar component of $Y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Variance-Covariance matrix\n",
    "\n",
    "- for a vector ${\\displaystyle \\mathbf {X} ={\\begin{bmatrix}X_{1}&X_{2}&\\dots &X_{m}\\end{bmatrix}}^{\\mathrm {T} }}$ of $m$ jointly distributed random variables \n",
    "- its covariance matrix (also known as the variance–covariance matrix) is\n",
    "\n",
    "\n",
    "$${\\displaystyle \\Sigma (\\mathbf {X} )=\\operatorname {cov} (\\mathbf {X} ,\\mathbf {X} )}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Properties of covariance\n",
    "\n",
    "- if $X$ is a random vector with covariance matrix $Σ(X)$, and $A$ is a matrix\n",
    "- then \n",
    "\n",
    "$${\\displaystyle \\Sigma (\\mathbf {A} \\mathbf {X} )=\\operatorname {E} [\\mathbf {A} \\mathbf {X} \\mathbf {X} ^{\\mathrm {T} }\\mathbf {A} ^{\\mathrm {T} }]-\\operatorname {E} [\\mathbf {A} \\mathbf {X} ]\\operatorname {E} [\\mathbf {X} ^{\\mathrm {T} }\\mathbf {A} ^{\\mathrm {T} }]=\\mathbf {A} \\Sigma (\\mathbf {X} )\\mathbf {A} ^{\\mathrm {T} }}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Covariance\n",
    "\n",
    "- the magnitude of the covariance is not easy to interpret \n",
    "  - it is not normalized and hence depends on the magnitudes of the variables\n",
    "- the normalized version of the covariance is called *correlation coefficient*\n",
    "  - the magnitude of the correlation coefficient shows the strength of the linear relation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Correlation: Pearson's product-moment coefficient\n",
    "\n",
    "- it is defined as the covariance of the two variables divided by the product of their standard deviations\n",
    "\n",
    "$$\\rho _{X,Y}=\\mathrm {corr} (X,Y)={\\mathrm {cov} (X,Y) \\over \\sigma _{X}\\sigma _{Y}}={E[(X-\\mu _{X})(Y-\\mu _{Y})] \\over \\sigma _{X}\\sigma _{Y}}$$\n",
    "\n",
    "- $\\rho$ is symmetric: corr(X,Y) = corr(Y,X)\n",
    "- $\\rho$ is +1 in the case of a perfect direct (increasing) linear relationship (correlation)\n",
    "- $\\rho$ is −1 in the case of a perfect decreasing (inverse) linear relationship (anticorrelation)\n",
    "- as $\\rho$ approaches zero there is less of a relationship (closer to uncorrelated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Independence\n",
    "\n",
    "- two events are independent if the occurrence of one does not affect the probability of occurrence of the other\n",
    "- two random variables are *independent* if the realization of one does not affect the probability distribution of the other\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Correlation vs dependency \n",
    "\n",
    "- random variables are *dependent* if they are not probabilistically independent\n",
    "- correlation refers to a specific *type* of dependency: \n",
    "<br>a *linear* relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Note\n",
    "\n",
    "if the variables are independent, then the correlation coefficient is 0, <br>but the converse is not true \n",
    "\n",
    "<center><img src=\"img/rho.png\" width=\"700\"/></center>\n",
    "\n",
    "\n",
    "only in the special case when X and Y are *jointly normal*\n",
    "<br> then <u>uncorrelatedness is equivalent to independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Independence\n",
    "\n",
    "- two random variables $X$ and $Y$ \n",
    "  - with cumulative distribution functions $F_X(x)$ and $F_Y(y)$ \n",
    "  - and probability densities $f_{X}(x)$ and $f_Y(y)$\n",
    "- are independent iff the combined random variable $(X, Y)$ has a joint cumulative distribution function $F_{X,Y}(x,y) = F_X(x) F_Y(y)$\n",
    "- or equivalently, if the joint density exists, $f_{X,Y}(x,y) = f_X(x) f_Y(y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Independence example\n",
    "\n",
    "- if two cards are drawn *with* replacement from a deck of cards\n",
    "  - the event of drawing a red card on the first trial and that of drawing a red card on the second trial \n",
    "- are *independent*\n",
    "\n",
    "\n",
    "- if two cards are drawn *without* replacement from a deck of cards\n",
    "  - the event of drawing a red card on the first trial and that of drawing a red card on the second trial\n",
    "- are *not independent*\n",
    "  - because a deck that has had a red card removed has proportionately fewer red cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Uncorrelatedness and independence\n",
    "\n",
    "\n",
    "- if $X$ and $Y$ are independent, then their covariance is zero\n",
    "- in fact independence means\n",
    "$${\\displaystyle \\operatorname {E} [XY]=\\operatorname {E} [X]\\cdot \\operatorname {E} [Y]}$$\n",
    "- and \n",
    "$${\\displaystyle {\\begin{aligned}\\operatorname {cov} (X,Y)&=\\operatorname {E} \\left[XY\\right]-\\operatorname {E} \\left[X\\right]\\operatorname {E} \\left[Y\\right]\\end{aligned}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Uncorrelatedness and independence\n",
    "\n",
    "\n",
    "- but if two variables are uncorrelated, that does not in general imply that they are independent\n",
    "- e.g. if  $X$ is uniformly distributed in [−1, 1] and $Y = X^2$\n",
    "  - $X$ and $Y$ are dependent, but\n",
    "\n",
    "$${\\displaystyle {\\begin{aligned}\\sigma (X,Y)&=\\sigma (X,X^{2})\\\\&=\\operatorname {E} [X\\cdot X^{2}]-\\operatorname {E} [X]\\cdot \\operatorname {E} [X^{2}]\\\\&=\\operatorname {E} \\!\\left[X^{3}\\right]-\\operatorname {E} [X]\\operatorname {E} [X^{2}]\\\\&=0-0\\cdot \\operatorname {E} [X^{2}]=0\\end{aligned}}}$$\n",
    "\n",
    "- the relationship between $Y$ and $X$ is non-linear, \n",
    "- while correlation and covariance are measures of *linear dependence* between two variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Correlation and dependence\n",
    "\n",
    "- a *dependence* or association is any statistical relationship between two random variables\n",
    "  - it can be *causal* or not\n",
    "\n",
    "- the *correlation* indicates a specific type of dependency: a *linear* relationship "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Correlation does not imply causation!\n",
    "\n",
    "- when two variables are correlated one could imagine that one variable causes the other\n",
    "- \"correlation proves causation\" is a logical *fallacy*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# B causes A (reverse causation)\n",
    "\n",
    "- the faster windmills are observed to rotate, the more wind is observed to be\n",
    "  - therefore wind is caused by the rotation of windmills\n",
    "- when a country's debt rises above 90% of GDP, growth slows\n",
    "  - therefore, high debt causes slow growth\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Third factor C (the common-causal variable) \n",
    "\n",
    "- sleeping with one's shoes on is strongly correlated with waking up with a headache\n",
    "  - therefore, sleeping with one's shoes on causes headache\n",
    "  - third factor: going to bed drunk\n",
    "- as ice cream sales increase, the rate of drowning deaths increases sharply\n",
    "  - therefore, ice cream consumption causes drowning\n",
    "  - third factor: in summer people swim more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# The relationship between A and B is coincidental\n",
    "\n",
    "- there is a 99.79% correlation for the period 1999-2009 between U.S. spending on science and the number of suicides by suffocation\n",
    "- a bald state leader of Russia has succeeded a hairy one for 200 years"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
