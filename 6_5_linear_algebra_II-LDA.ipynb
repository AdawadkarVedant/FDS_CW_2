{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href=\"https://fonts.googleapis.com/css?family=Cabin|Quicksand\" rel=\"stylesheet\"><style>.container{width:90% !important; font-family: \"Cabin\", sans-serif;}em{color: red !important;}</style><style>.output_png {display: table-cell;text-align: center;vertical-align: middle;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sympy import init_printing \n",
    "from sympy import Matrix\n",
    "init_printing(use_latex=True)\n",
    "def out(mat, n=2): return Matrix(np.round(mat, decimals=n))\n",
    "from IPython.core.display import HTML\n",
    "HTML('<link href=\"https://fonts.googleapis.com/css?family=Cabin|Quicksand\" rel=\"stylesheet\"><style>.container{width:90% !important; font-family: \"Cabin\", sans-serif;}em{color: red !important;}</style><style>.output_png {display: table-cell;text-align: center;vertical-align: middle;}</style>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear discriminant analysis (LDA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear discriminant analysis (LDA)\n",
    "\n",
    "- also called normal discriminant analysis (NDA)\n",
    "- is a procedure to determine a good linear combination of features that best separates two classes of instances\n",
    "- the resulting linear combination can be used as a (linear) *classifier* or for *dimensionality reduction*\n",
    "\n",
    "<center><img src=\"img/lda1.png\" alt=\"lda\" width=\"400\"/></center>\n",
    "\n",
    "[ref](https://eigenfoo.xyz/lda/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# LDA vs PCA\n",
    "\n",
    "- both LDA and PCA find linear combinations of variables to better \"understand\" a data matrix\n",
    "- LDA models the difference in data that belongs to different classes (*supervised* method)\n",
    "- PCA models the variability of data and does not consider any class information (*unsupervised* method)\n",
    "\n",
    "<center><img src=\"img/lda.png\" alt=\"lda\" width=\"700\"/></center>\n",
    "\n",
    "[ref](https://sebastianraschka.com/Articles/2014_python_lda.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# LDA interpretations\n",
    "\n",
    "- There are two ways to see how LDA acts\n",
    "\n",
    "1. **a geometric view**: LDA finds the direction such that, when we project all data on that direction, we obtain two distributions where their mean is well separated *and* their variance is minimised  \n",
    "2. **a probabilistic view** (we will look into this one): LDA finds an algebraic criterion that maximises the probability that instances are correctly classified as belonging to their group (under certain assumptions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# LDA geometric interpretation\n",
    "\n",
    "<center><img src=\"img/lda4.png\" alt=\"lda\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# LDA \n",
    "\n",
    "- classification problem:\n",
    "  - we are given data instances belonging to two classes (training set), each assumed to be distributed normally with mean $\\mu_0$ and   $\\mu_1$ and covariances $\\Sigma_0$ and $\\Sigma_1$\n",
    "  - we want to make a good prediction for the class of instances provided without class information (test set)\n",
    "\n",
    "- LDA approach:\n",
    "  - it is possible to compute the probability that an unclassified instance belongs to one class or the other (i.e., one distribution or the other) by means of the Bayes Theorem and assign to the instance the class with higher probability \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# LDA (derivation)\n",
    "\n",
    "We can compute the probability of sampling an instance from a given class:\n",
    "- $P(X=x|C=0)$: probability of sampling the instance $x$ from the probability distribution $C=0$ (multivariate Gaussian density formula with $\\mu_0$, $\\Sigma_0$) \n",
    "- $P(X=x|C=1)$: probability of sampling the instance $x$ from the probability distribution $C=1$ (with params $\\mu_1$, $\\Sigma_1$)\n",
    "\n",
    "But we want the *posterior* probability that an instance originated from a certain class:\n",
    "- $P(C=0|X=x)$: probability of $C=0$ being the class the instance $x$ came from\n",
    "- $P(C=1|X=x)$: probability of $C=1$ being the class the instance $x$ came from\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# LDA (derivation)\n",
    "\n",
    "The posterior probabilities can be computed using the Bayes theorem for e.g. for $C=0$:\n",
    "\n",
    "$$P(C=0|X=x) = \\frac{P(X=x|C=0) \\cdot P(C=0)}{P(X=x)}$$\n",
    "\n",
    "We need the *prior* probabilities that we can estimate from data:\n",
    "- $P(C=0)$: probability of sampling a point of class $C=0$ (relative frequency of class 0)\n",
    "- $P(C=1)$: probability of sampling a point of class $C=1$ (relative frequency of class 1)\n",
    "- $P(X=x)$: probability of sampling a point (=1 as we are actually sampling with certainty)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# LDA (derivation)\n",
    "\n",
    "- what is the best way to assign a class to a novel instance $x$?\n",
    "- if $P(C=0|X=x) > P(C=1|X=x)$ we will assign class 0, otherwise class 1 (discriminating using posterior probabilities: given a specific $x$, probabilities of $x$ from $C=0$ or $C=1$)\n",
    "- so we want to find out for which $x$ the following condition holds:\n",
    "$$\\begin{equation}\n",
    "\\frac{P(C=0|X=x)}{P(C=1|X=x)} > 1 \\\\ \n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# LDA (derivation)\n",
    "\n",
    "If we *assume* that $\\Sigma_0 = \\Sigma_1 = \\Sigma$, substituting the multivariate Gaussian density formula and simplifying, we get the  expression:\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\log \\frac{\\pi_0}{\\pi_1} - \\frac{1}{2} {\\mu}_0^T {\\Sigma}^{-1} {\\mu}_0 + \\frac{1}{2} {\\mu}_1^T {\\Sigma}^{-1} {\\mu}_1 + x^T \\Sigma^{-1} (\\mu_0 - \\mu_1) > 0\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "which is an equation linear in $x$, of the form\n",
    "\n",
    "$$  x^T w + c > 0$$\n",
    "\n",
    "where\n",
    "$$\\begin{equation}\n",
    "w = \\Sigma^{-1} (\\mu_0 - \\mu_1) \\\\\n",
    "c =  \\log \\frac{\\pi_0}{\\pi_1} - \\frac{1}{2} {\\mu}_0^T {\\Sigma}^{-1} {\\mu}_0 + \\frac{1}{2} {\\mu}_1^T {\\Sigma}^{-1} {\\mu}_1 \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# LDA (estimation of parameters)\n",
    "\n",
    "- to estimate $w$ and $c$ we need to estimate the parameters of the Gaussian distribution $\\pi_0$, $\\pi_1$, $\\mu_0$, $\\mu_1$ and $\\Sigma$ from data\n",
    "  - $\\hat{\\pi}_0 = \\frac{N_0}{N}$ where $N_0$ is the number of observations in class 0 and $N$ the total number of observations ($\\hat{\\pi}_1 = \\frac{N_1}{N}$)\n",
    "  - $\\hat{\\mu}_0 = \\frac{1}{N_0} \\sum_{C_i=0} x_i$ where $C_i$ is the class of instance $i$, i.e. mean vector of instances of class $0$ ($\\hat{\\mu}_1 = \\frac{1}{N_1} \\sum_{C_i=1} x_i$)\n",
    "  - $\\hat{\\Sigma} = \\frac{1}{2} (\\hat{\\Sigma_0}+\\hat{\\Sigma_1})$ where $\\hat{\\Sigma_0}$ and $\\hat{\\Sigma_1}$ are the sample covariances of the instances belonging to the two classes considered separately, i.e. we compute the *average covariance matrix*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# LDA (estimation of parameters)\n",
    "\n",
    "- the LDA rule classifies $x$ to belong to class 0 if  \n",
    "$$ x^T w > - c$$\n",
    "$$ x^T \\hat{\\Sigma}^{-1} (\\hat{\\mu}_0 - \\hat{\\mu}_1) > \\frac{1}{2} \\hat{\\mu}_0^T \\hat{\\Sigma}^{-1} \\hat{\\mu}_0 - \\frac{1}{2} \\hat{\\mu}_1^T \\hat{\\Sigma}^{-1} \\hat{\\mu}_1 + \\log(N_1/N) - \\log(N_0/N)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# LDA \n",
    "\n",
    "- the equation $x^T w + c = 0$ defines a linear decision surface (a hyperplane)\n",
    "- the vector $w$ is orthogonal to the decision surface\n",
    "- the offset $c$ decides the position of the decision surface\n",
    "\n",
    "<center><img src=\"img/lda2.png\" alt=\"lda\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# LDA \n",
    "\n",
    "- when there are more than 2 classes, to classify a point one can consider all pairwise comparisons and decide the class by majority voting\n",
    "- the covariance matrix is computed as the average covariance matrix\n",
    "\n",
    "<center><img src=\"img/lda3.png\" alt=\"lda\" width=\"700\"/></center>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
